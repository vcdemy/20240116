{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vcdemy/20240116/blob/main/20240123%E7%AD%86%E8%A8%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7816912d-165e-493c-b352-304427b7fe6b",
      "metadata": {
        "id": "7816912d-165e-493c-b352-304427b7fe6b"
      },
      "source": [
        "# 20240123 筆記"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac0b727b-722d-4ae8-ad43-54f33bf2b17b",
      "metadata": {
        "tags": [],
        "id": "ac0b727b-722d-4ae8-ad43-54f33bf2b17b",
        "outputId": "ed19533d-4255-445d-8ced-8fc22755f7a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\victor\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from keras.models import load_model  # TensorFlow is required for Keras to work\n",
        "from PIL import Image, ImageOps  # Install pillow instead of PIL\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8477ede-9368-4258-ba28-b66eb69ff497",
      "metadata": {
        "tags": [],
        "id": "f8477ede-9368-4258-ba28-b66eb69ff497",
        "outputId": "8068a061-cefb-418a-ffb3-4763079a5f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\victor\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\victor\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "1/1 [==============================] - 1s 976ms/step\n",
            "Class: Glass\n",
            "Confidence Score: 0.7306934\n"
          ]
        }
      ],
      "source": [
        "# Disable scientific notation for clarity\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Load the model\n",
        "model = load_model(\"keras_model.h5\", compile=False)\n",
        "\n",
        "# Load the labels\n",
        "class_names = open(\"labels.txt\", \"r\").readlines()\n",
        "\n",
        "# Create the array of the right shape to feed into the keras model\n",
        "# The 'length' or number of images you can put into the array is\n",
        "# determined by the first position in the shape tuple, in this case 1\n",
        "data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
        "\n",
        "# Replace this with the path to your image\n",
        "image = Image.open(\"data/glass/glass110.jpg\").convert(\"RGB\")\n",
        "\n",
        "# resizing the image to be at least 224x224 and then cropping from the center\n",
        "size = (224, 224)\n",
        "image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)\n",
        "\n",
        "# turn the image into a numpy array\n",
        "image_array = np.asarray(image)\n",
        "\n",
        "# Normalize the image\n",
        "normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1\n",
        "\n",
        "# Load the image into the array\n",
        "data[0] = normalized_image_array\n",
        "\n",
        "# Predicts the model\n",
        "prediction = model.predict(data)\n",
        "index = np.argmax(prediction)\n",
        "class_name = class_names[index]\n",
        "confidence_score = prediction[0][index]\n",
        "\n",
        "# Print prediction and confidence score\n",
        "print(\"Class:\", class_name[2:], end=\"\")\n",
        "print(\"Confidence Score:\", confidence_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0916ea7-8965-498c-9cd7-bfaa179a1cd2",
      "metadata": {
        "tags": [],
        "id": "e0916ea7-8965-498c-9cd7-bfaa179a1cd2"
      },
      "outputs": [],
      "source": [
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5b310fe-fd18-4a29-b993-900da21c83a8",
      "metadata": {
        "tags": [],
        "id": "a5b310fe-fd18-4a29-b993-900da21c83a8"
      },
      "outputs": [],
      "source": [
        "def predict(image):\n",
        "    # Disable scientific notation for clarity\n",
        "    np.set_printoptions(suppress=True)\n",
        "\n",
        "    # Load the model\n",
        "    model = load_model(\"keras_model.h5\", compile=False)\n",
        "\n",
        "    # Load the labels\n",
        "    class_names = open(\"labels.txt\", \"r\").readlines()\n",
        "\n",
        "    # Create the array of the right shape to feed into the keras model\n",
        "    # The 'length' or number of images you can put into the array is\n",
        "    # determined by the first position in the shape tuple, in this case 1\n",
        "    data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
        "\n",
        "    # Replace this with the path to your image\n",
        "    # image = Image.open(\"data/glass/glass110.jpg\").convert(\"RGB\")\n",
        "\n",
        "    # resizing the image to be at least 224x224 and then cropping from the center\n",
        "    size = (224, 224)\n",
        "    image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    # turn the image into a numpy array\n",
        "    image_array = np.asarray(image)\n",
        "\n",
        "    # Normalize the image\n",
        "    normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1\n",
        "\n",
        "    # Load the image into the array\n",
        "    data[0] = normalized_image_array\n",
        "\n",
        "    # Predicts the model\n",
        "    prediction = model.predict(data)\n",
        "    index = np.argmax(prediction)\n",
        "    class_name = class_names[index]\n",
        "    confidence_score = prediction[0][index]\n",
        "\n",
        "    # Print prediction and confidence score\n",
        "    print(\"Class:\", class_name[2:], end=\"\")\n",
        "    print(\"Confidence Score:\", confidence_score)\n",
        "    return class_name[2:], confidence_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "891df208-7e0b-45e6-b83e-ffd596eb74d1",
      "metadata": {
        "tags": [],
        "id": "891df208-7e0b-45e6-b83e-ffd596eb74d1",
        "outputId": "e06d3af6-7944-456c-d94d-50c55d39ea05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Class: cardboard\n",
            "Confidence Score: 0.99991465\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Class: Plastic\n",
            "Confidence Score: 0.9992405\n"
          ]
        }
      ],
      "source": [
        "gr.Interface(predict, gr.Image(type='pil'), ['text', 'text']).launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8545e9c9-05b7-4f6c-bcf8-8a6ee6f45653",
      "metadata": {
        "tags": [],
        "id": "8545e9c9-05b7-4f6c-bcf8-8a6ee6f45653"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da699214-f73d-4b11-a8d4-6e69089a6bce",
      "metadata": {
        "tags": [],
        "id": "da699214-f73d-4b11-a8d4-6e69089a6bce"
      },
      "outputs": [],
      "source": [
        "x = np.array([[1, 2], [3, 4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d0bcf0f-6032-4647-beec-5958f361511d",
      "metadata": {
        "tags": [],
        "id": "6d0bcf0f-6032-4647-beec-5958f361511d",
        "outputId": "5913ac5b-daa2-4fee-b6b7-2d3ef5f839bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [3, 4]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c01b2102-aa93-4291-a3f6-4c8ca1dd0d38",
      "metadata": {
        "tags": [],
        "id": "c01b2102-aa93-4291-a3f6-4c8ca1dd0d38",
        "outputId": "25c850d4-d051-4614-b6d9-5fc5fccf9270"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 2)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "317428df-c09f-44de-a08c-811776b822b4",
      "metadata": {
        "tags": [],
        "id": "317428df-c09f-44de-a08c-811776b822b4",
        "outputId": "684e6dbf-c357-4e7f-f1d1-15e9659fa705"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[1, 2],\n",
              "        [3, 4]]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.reshape((1, 2, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a271c41-3ba3-4f5e-bed1-4210cb6ee1a2",
      "metadata": {
        "tags": [],
        "id": "9a271c41-3ba3-4f5e-bed1-4210cb6ee1a2"
      },
      "outputs": [],
      "source": [
        "y = x.reshape((1, 2, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208e6cfd-6712-48e7-afef-dbbbf2805db4",
      "metadata": {
        "tags": [],
        "id": "208e6cfd-6712-48e7-afef-dbbbf2805db4",
        "outputId": "69d096da-b344-4136-abb9-86c426eb9993"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 2])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3495fc9f-37d1-4d80-82a9-24be1707e35e",
      "metadata": {
        "tags": [],
        "id": "3495fc9f-37d1-4d80-82a9-24be1707e35e",
        "outputId": "86502202-6985-42c6-c422-96f234d80809"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [3, 4]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d74776-87b2-440b-ada6-b05297a2c38e",
      "metadata": {
        "tags": [],
        "id": "83d74776-87b2-440b-ada6-b05297a2c38e",
        "outputId": "79d34751-3e18-4b4e-f83f-464c0e081e0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [3, 4]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6dd52b-9687-4bda-ac53-b0be2bc498ef",
      "metadata": {
        "tags": [],
        "id": "bb6dd52b-9687-4bda-ac53-b0be2bc498ef"
      },
      "outputs": [],
      "source": [
        "x1 = x*2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b27b8d-15c0-4094-8be2-0f6ee77a80fe",
      "metadata": {
        "tags": [],
        "id": "14b27b8d-15c0-4094-8be2-0f6ee77a80fe"
      },
      "outputs": [],
      "source": [
        "x2 = x*3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc700d84-d8ec-49d6-a841-c7dcdb477e39",
      "metadata": {
        "tags": [],
        "id": "fc700d84-d8ec-49d6-a841-c7dcdb477e39"
      },
      "outputs": [],
      "source": [
        "z = np.array([x1, x2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d24fb2c-32c2-47e3-8718-5d3a7272374e",
      "metadata": {
        "tags": [],
        "id": "3d24fb2c-32c2-47e3-8718-5d3a7272374e",
        "outputId": "09443237-acb2-464f-8d9f-14df5078fa5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 2, 2)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9752cba-484a-40e0-84a3-5c3e09d3c71d",
      "metadata": {
        "id": "f9752cba-484a-40e0-84a3-5c3e09d3c71d"
      },
      "source": [
        "# YOLO v5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a91f3a31-603e-44ac-84dc-c6f8596b1574",
      "metadata": {
        "id": "a91f3a31-603e-44ac-84dc-c6f8596b1574"
      },
      "source": [
        "https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/#simple-example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3512601-1b7b-448a-8f42-27fda5c7a88b",
      "metadata": {
        "tags": [],
        "id": "c3512601-1b7b-448a-8f42-27fda5c7a88b"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eec11a1-f592-47a4-a9b0-4e6033448178",
      "metadata": {
        "id": "7eec11a1-f592-47a4-a9b0-4e6033448178"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "\n",
        "# Images\n",
        "for f in 'zidane.jpg', 'bus.jpg':\n",
        "    torch.hub.download_url_to_file('https://ultralytics.com/images/' + f, f)  # download 2 images\n",
        "im1 = Image.open('zidane.jpg')  # PIL image\n",
        "im2 = cv2.imread('bus.jpg')[..., ::-1]  # OpenCV image (BGR to RGB)\n",
        "\n",
        "# Inference\n",
        "results = model([im1, im2], size=640)  # batch of images\n",
        "\n",
        "# Results\n",
        "results.print()\n",
        "results.save()  # or .show()\n",
        "\n",
        "results.xyxy[0]  # im1 predictions (tensor)\n",
        "results.pandas().xyxy[0]  # im1 predictions (pandas)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}